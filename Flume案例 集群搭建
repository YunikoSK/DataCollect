Flume集群搭建 使用 hadoop3 为master 使用 hadoop2为 slave1  hadoop3 为 slave2
hdfs
----hadoop1
----hadoop2
----hadoop3
hadoop3为master 
 hadoop1为slave1
 hadoop2为slave2

在hadoop3上 
first-masterjob
-----文件内容 
a1.sources=r1
a1.channels=c1
a1.sinks=s1
a1.sources.r1.type=avro
a1.sources.r1.port=6666
a1.sources.r1.bind= 192.168.248.132
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100
a1.channels.c1.keep-alive=3
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000
a1.sinks.s1.type =logger
a1.sources.r1.channels=c1
a1.sinks.s1.channel=c1
-------------------------------------------------------------------------
在hadoop2上
first-slave2ob
a1.sources=r1
a1.channels=c1
a1.sinks=s1
a1.sources.r1.type=syslogtcp
a1.sources.r1.port=6666
a1.sources.r1.host=hadoop2
a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100
a1.channels.c1.keep-alive=3
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000
a1.sinks.s1.type =avro
a1.sinks.s1.hostname=hadoop3
a1.sinks.s1.port=6666
a1.sources.r1.channels=c1
a1.sinks.s1.channel=c1
----------------------------------------------------------------------------
在hadoop1机器上
first-slave1job
a1.sources=r1
a1.channels=c1
a1.sinks=s1

a1.sources.r1.type=syslogtcp
a1.sources.r1.port=6666
a1.sources.r1.host=hadoop1

a1.channels.c1.type=memory
a1.channels.c1.capacity=1000
a1.channels.c1.transactionCapacity=100
a1.channels.c1.keep-alive=3
a1.channels.c1.byteCapacityBufferPercentage = 20
a1.channels.c1.byteCapacity = 800000

a1.sinks.s1.type = avro
a1.sinks.s1.hostname = hadoop3
a1.sinks.s1.port = 6666

a1.sources.r1.channels=c1
a1.sinks.s1.channel=c1

---------------------------------------------------------------------------
启动
必须先启动 master
 ../bin/flume-ng agent -c ../conf/ -f ./first-masterjob -n a1 -Dflume.root.logger=INFO,console
然后启动slave1 slave2
hadoop1
 ../bin/flume-ng agent -c ../conf/ -f ./first-slave1job -n a1 -Dflume.root.logger=INFO,console &
hadoop2
 ../bin/flume-ng agent -c ../conf/ -f ./first-slave2job -n a1 -Dflume.root.logger=INFO,console &
在hadoop1机器上 向 本机的端口6666发送 
 echo "nihao hadoop3 woshi hadoop1 " | nc localhost 6666
 echo "nihao hadoop3 woshi hadoop2 " | nc localhost 6666
观察hadoop3的日志接收情况 



