使用datax的实现datax的和mysql的数据互导
--------------------------------------------------------------------------
hive 建表语句
create table sqooppar7(
id int,
name string,
hoddy string
)
row format delimited fields terminated by ' ';

mysql 建表语句
create  table tt1 
(id int,
name varchar(10),
sal varchar(10))
mysql 存储过程 实现创建多条数据
BEGIN
DECLARE count INT(11);
DECLARE i INT(11);
DECLARE name VARCHAR(20);
DECLARE sal VARCHAR(20);

SET count = 100;
SET i =0;
WHILE i < count DO
SET name = CONCAT('1510A',i);
SET sal = ROUND(ROUND(RAND(),2)*100,2);
INSERT INTO `tt`(`NAME`,sal) VALUES (name,sal);
SET i = i+1;
END WHILE;
END



--------------------------mysql 到 hive --------------------------------

{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [ "id","name","sal",
                        "connection": [
                            {
                                "jdbcUrl": ["jdbc:mysql://hadoop1:3306/test"],
                                "table": ["tt1"]
                            }
                        ],
                        "password": "root",
                        "username": "root",
                        "where": ""
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [

                                   {
                                    "name":"id",
                                    "type":"int"
                                   },
                                   {
                                     "name":"name",
                                      "type":"string"
                                    },
                                    {
                                       "name":"hoddy",
                                        "type":"string"
                                     }
                                   ],
                        "compress": "",
                        "defaultFS": "hdfs://hadoop2:9000",
                        "fieldDelimiter": " ",
                        "fileName": "sqooppar7",
                        "fileType": "text",
                        "path": "/user/hive/warehouse/sqooppar7",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}


---------------------------------hive到mysql----------------------------------------
{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "column": [
                               {
                                "index": 0,
                                "type": "long"
                               },
                               {
                                "index": 1,
                                "type": "string"
                               },
                               {
                                "index":2,
                                "type": "string",

                               }],

                        "defaultFS": "hdfs://hadoop2:9000",
                        "encoding": "UTF-8",
                        "fieldDelimiter": " ",
                        "fileType": "text",
                        "path": "/user/hive/warehouse/sqooppar7"
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "column": ["id", "name","sal"],
                        "connection": [
                            {
                                 "jdbcUrl": "jdbc:mysql://hadoop1:3306/test",
                                "table": ["tt1"]
                            }
                        ],
                        "password": "root",
                        "preSql": [],
                        "session": [],
                        "username": "root",
                        "writeMode": "insert"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": "1"
            }
        }
    }
}
